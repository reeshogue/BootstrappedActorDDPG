# BootstrappedActorDDPG
BADDPG is an RL algorithm inspired by Adaptive Computation Time (https://arxiv.org/abs/1603.08983) and Bootstrap Your Own Latent (https://arxiv.org/abs/2006.07733). The ideas presented are simple and straightforward. During action generation, the actor network becomes it's own policy iteration algorithm, taking in a state and an action, and trying to map them to an even better action. We iterate this over k steps where k is a randomized integer from 1 to n. We then train the actor with the usual DDPG Q-learning loss and a special action computational compression loss, which attempts to reduce the number of computational steps required to reach the action in the replay buffer, given the state, in a way that's similar in style to Self-Imitation Learning (https://arxiv.org/abs/1806.05635). We believe this methodology is expandable to other areas of deep learning due to its overall simplicity.
